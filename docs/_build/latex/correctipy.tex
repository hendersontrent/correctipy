%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
  \ifdefined\DeclareUnicodeCharacterAsOptional
    \def\sphinxDUC#1{\DeclareUnicodeCharacter{"#1}}
  \else
    \let\sphinxDUC\DeclareUnicodeCharacter
  \fi
  \sphinxDUC{00A0}{\nobreakspace}
  \sphinxDUC{2500}{\sphinxunichar{2500}}
  \sphinxDUC{2502}{\sphinxunichar{2502}}
  \sphinxDUC{2514}{\sphinxunichar{2514}}
  \sphinxDUC{251C}{\sphinxunichar{251C}}
  \sphinxDUC{2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}



\usepackage{times}
\expandafter\ifx\csname T@LGR\endcsname\relax
\else
% LGR was declared as font encoding
  \substitutefont{LGR}{\rmdefault}{cmr}
  \substitutefont{LGR}{\sfdefault}{cmss}
  \substitutefont{LGR}{\ttdefault}{cmtt}
\fi
\expandafter\ifx\csname T@X2\endcsname\relax
  \expandafter\ifx\csname T@T2A\endcsname\relax
  \else
  % T2A was declared as font encoding
    \substitutefont{T2A}{\rmdefault}{cmr}
    \substitutefont{T2A}{\sfdefault}{cmss}
    \substitutefont{T2A}{\ttdefault}{cmtt}
  \fi
\else
% X2 was declared as font encoding
  \substitutefont{X2}{\rmdefault}{cmr}
  \substitutefont{X2}{\sfdefault}{cmss}
  \substitutefont{X2}{\ttdefault}{cmtt}
\fi


\usepackage[Bjarne]{fncychap}
\usepackage{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\addto\captionsenglish{\renewcommand{\contentsname}{Contents:}}

\usepackage{sphinxmessages}
\setcounter{tocdepth}{1}



\title{correctipy}
\date{Jan 12, 2023}
\release{0.1.0}
\author{Trent Henderson}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}



\chapter{Quickstart}
\label{\detokenize{quickstart:quickstart}}\label{\detokenize{quickstart::doc}}

\section{Installation}
\label{\detokenize{quickstart:installation}}
You can install correctipy using pip

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pip} \PYG{n}{install} \PYG{n}{git}\PYG{o}{+}\PYG{n}{https}\PYG{p}{:}\PYG{o}{/}\PYG{o}{/}\PYG{n}{github}\PYG{o}{.}\PYG{n}{com}\PYG{o}{/}\PYG{n}{hendersontrent}\PYG{o}{/}\PYG{n}{correctipy}
\end{sphinxVerbatim}


\section{Usage}
\label{\detokenize{quickstart:usage}}
In the real world, we would have proper results obtained through fitting two models according to one or more of the procedures contained in correctipy (random subsampling, k\sphinxhyphen{}fold cross validation, repeated k\sphinxhyphen{}fold cross\sphinxhyphen{}validation). For simplicity here, we are just going to simulate three datasets so we can get to the package functionality cleaner and easier. We are going to assume we are in a classification context and generate classification accuracy values. These values are purposefully egregious—we are going to (in the case of the random subsampling) just fix the train set sample size (\sphinxcode{\sphinxupquote{n1}}) to 80 and the test set sample size (\sphinxcode{\sphinxupquote{n2}}) to 20, and assume (using the same data) for the \$k\$\sphinxhyphen{}fold cross\sphinxhyphen{}validation correction that the same numbers were obtained on such a method. Again, the values are not important here, it is the corrections we are going to apply next that are crucial.

In the case of repeated \$k\$\sphinxhyphen{}fold cross\sphinxhyphen{}validation, take note of the column names. While your dataframe you pass in to \sphinxcode{\sphinxupquote{repkfold\_ttest}} can have more than the four columns specified here, it must contain at least these four with the exact corresponding names. The function explicitly searches for them. They are:
\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{"model"}} — contains a label for each of the two models to compare

\item {} 
\sphinxcode{\sphinxupquote{"values"}} — the numerical values of the performance metric (i.e., classification accuracy)

\item {} 
\sphinxcode{\sphinxupquote{"k"}} — which fold the values correspond to

\item {} 
\sphinxcode{\sphinxupquote{"r"}} — which repeat of the fold the values correspond to

\end{itemize}

Here is the simulated data:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{x} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{l+m+mf}{0.6}\PYG{p}{,} \PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{l+m+mi}{30}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{y} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{l+m+mf}{0.4}\PYG{p}{,} \PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{l+m+mi}{30}\PYG{p}{)}

\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{tmp} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{model}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{n}{np}\PYG{o}{.}\PYG{n}{repeat}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,} \PYG{l+m+mi}{60}\PYG{p}{)}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }                   \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{values}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{n}{np}\PYG{o}{.}\PYG{n}{concatenate}\PYG{p}{(}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{l+m+mf}{0.6}\PYG{p}{,} \PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{l+m+mi}{60}\PYG{p}{)}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }                   \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{l+m+mf}{0.4}\PYG{p}{,} \PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{l+m+mi}{60}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }                   \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{k}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{*}\PYG{l+m+mi}{30}\PYG{p}{,}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }                   \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{r}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{*}\PYG{l+m+mi}{60}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }                  \PYG{p}{\PYGZcb{}}\PYG{p}{)}
\end{sphinxVerbatim}

We can fit all the corrections in one\sphinxhyphen{}line functions:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{correctipy} \PYG{k+kn}{import} \PYG{n}{resampled\PYGZus{}ttest}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{correctipy} \PYG{k+kn}{import} \PYG{n}{kfold\PYGZus{}ttest}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{correctipy} \PYG{k+kn}{import} \PYG{n}{repkfold\PYGZus{}ttest}

\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{rss} \PYG{o}{=} \PYG{n}{resampled\PYGZus{}ttest}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{l+m+mi}{30}\PYG{p}{,} \PYG{l+m+mi}{80}\PYG{p}{,} \PYG{l+m+mi}{20}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} Random subsampling}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{kcv} \PYG{o}{=} \PYG{n}{kfold\PYGZus{}ttest}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{l+m+mi}{100}\PYG{p}{,} \PYG{l+m+mi}{30}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} k\PYGZhy{}fold cross\PYGZhy{}validation}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{kcv} \PYG{o}{=} \PYG{n}{kfold\PYGZus{}ttest}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{l+m+mi}{100}\PYG{p}{,} \PYG{l+m+mi}{30}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} k\PYGZhy{}fold cross\PYGZhy{}validation}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{rkcv} \PYG{o}{=} \PYG{n}{repkfold\PYGZus{}ttest}\PYG{p}{(}\PYG{n}{tmp}\PYG{p}{,} \PYG{l+m+mi}{80}\PYG{p}{,} \PYG{l+m+mi}{20}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} Repeated k\PYGZhy{}fold cross\PYGZhy{}validation}
\end{sphinxVerbatim}

All the functions return a Pandas dataframe with two named columns: \sphinxcode{\sphinxupquote{"statistic"}} (the t\sphinxhyphen{}statistic) and \sphinxcode{\sphinxupquote{"p\_value"}} (the associated p\sphinxhyphen{}value), meaning they can be easily integrated into complex machine pipelines. Here is an example for the \sphinxcode{\sphinxupquote{resampled\_ttest}} case:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{rss}\PYG{p}{)}

\PYG{g+go}{       statistic       p\PYGZus{}value}
\PYG{g+go}{  0    6.09829  6.083703e\PYGZhy{}07}
\end{sphinxVerbatim}


\chapter{Test statistic mathematics}
\label{\detokenize{statistic_info:test-statistic-mathematics}}\label{\detokenize{statistic_info::doc}}
Often in machine learning, we want to compare the performance of
different models to determine if one statistically outperforms another.
However, the methods used (e.g., data resampling, k\sphinxhyphen{}fold
cross\sphinxhyphen{}validation) to obtain these performance metrics (e.g.,
classification accuracy) violate the assumptions of traditional
statistical tests such as a t\sphinxhyphen{}test. The purpose of these methods is to
either aid generalisability of findings (i.e., through quantification of
error as they produce multiple values for each model instead of just
one) or to optimise model hyperparameters. This makes them invaluable,
but unusable with traditional tests, as \sphinxhref{https://pubmed.ncbi.nlm.nih.gov/9744903/}{Dietterich
(1998)} found that the
standard t\sphinxhyphen{}test underestimates the variance, therefore driving a high
Type I error. correctipy is a lightweight package that implements a
small number of corrected test statistics for cases when samples are not
independent (and therefore are correlated), such as in the case of
resampling, k\sphinxhyphen{}fold cross\sphinxhyphen{}validation, and repeated k\sphinxhyphen{}fold
cross\sphinxhyphen{}validation. These corrections were all originally proposed by \sphinxhref{https://link.springer.com/article/10.1023/A:1024068626366}{Nadeau and Bengio
(2003)}.
Currently, only cases where two models are to be compared are supported.

correctipy is a lightweight package that implements a small number of corrected test statistics for cases when samples of two machine learning model metrics (e.g., classification accuracy) are not independent (and therefore are correlated), such as in the case of resampling and k\sphinxhyphen{}fold cross\sphinxhyphen{}validation. We demonstrate the basic functionality here using some trivial examples for the following corrected tests that are currently implemented in correctipy:
\begin{itemize}
\item {} 
Random subsampling

\item {} 
k\sphinxhyphen{}fold cross\sphinxhyphen{}validation

\item {} 
Repeated k\sphinxhyphen{}fold cross\sphinxhyphen{}validation

\end{itemize}

These corrections were all originally proposed by Nadeau and Bengio (2003) with additional representations in \sphinxhref{https://link.springer.com/chapter/10.1007/978-3-540-24775-3\_3}{Bouckaert and Frank (2004)}.


\section{Random subsampling correction}
\label{\detokenize{statistic_info:random-subsampling-correction}}
In random subsampling, the standard t\sphinxhyphen{}test inflates Type I error when used in conjunction with random subsampling due to an underestimation of the variance, as found by Dietterich (1998). Nadeau and Bengio (2003) proposed a solution (which we implement as \sphinxcode{\sphinxupquote{correctipy.resampled\_ttest()}} in correctipy) in the form of:
\begin{equation*}
\begin{split}t = \frac{\frac{1}{n} \sum_{j=1}^{n}x_{j}}{\sqrt{(\frac{1}{n} + \frac{n_{2}}{n_{1}})\sigma^{2}}}\end{split}
\end{equation*}
where \(n\) is the number of resamples (NOTE: \(n\) is not sample size), \(n_{1}\) is the number of samples in the training data, and \(n_{2}\) is the number of samples in the test data. \(\\sigma^{2}\) is the variance estimate used in the standard paired t\sphinxhyphen{}test (which simply has \(\\frac{\\sigma}{\\sqrt{n}}\) in the denominator where \(n\) is the sample size in this case).


\section{k\sphinxhyphen{}fold cross\sphinxhyphen{}validation correction}
\label{\detokenize{statistic_info:k-fold-cross-validation-correction}}
There is an alternate formulation of the random subsampling correction, devised in terms of the unbiased estimator \(\\rho\), discussed in \sphinxhref{https://link.springer.com/article/10.1007/s10994-017-5641-9}{Corani et al. (2016)} which we implement as \sphinxcode{\sphinxupquote{correctipy.kfold\_tttest()}} in correctipy:
\begin{equation*}
\begin{split}t = \frac{\frac{1}{n} \sum_{j=1}^{n}x_{j}}{\sqrt{(\frac{1}{n} + \frac{\rho}{1-\rho})\sigma^{2}}}\end{split}
\end{equation*}
where \(n\) is the number of resamples and \(\\rho = \\frac{1}{k}\) where \(k\) is the number of folds in the k\sphinxhyphen{}fold cross\sphinxhyphen{}validation procedure. This formulation stems from the fact that Nadeau and Bengio (2003) proved there is no unbiased estimator, but it can be approximated with \(\\rho = \\frac{1}{k}\).


\section{Repeated k\sphinxhyphen{}fold cross\sphinxhyphen{}validation correction}
\label{\detokenize{statistic_info:repeated-k-fold-cross-validation-correction}}
Repeated k\sphinxhyphen{}fold cross\sphinxhyphen{}validation is more complex than the previous case(s) as we now have r repeats for every fold k. Bouckaert and Frank (2004) present a nice representation of the corrected test for this case which we implement as \sphinxcode{\sphinxupquote{correctipy.repkfold\_ttest()}} in correctipy:
\begin{equation*}
\begin{split}t = \frac{\frac{1}{k \cdot r} \sum_{i=1}^{k} \sum_{j=1}^{r} x_{ij}}{\sqrt{(\frac{1}{k \cdot r} + \frac{n_{2}}{n_{1}})\sigma^{2}}}\end{split}
\end{equation*}

\chapter{Indices and tables}
\label{\detokenize{index:indices-and-tables}}\begin{itemize}
\item {} 
\DUrole{xref,std,std-ref}{genindex}

\item {} 
\DUrole{xref,std,std-ref}{modindex}

\item {} 
\DUrole{xref,std,std-ref}{search}

\end{itemize}



\renewcommand{\indexname}{Index}
\printindex
\end{document}